HTTP 库 requests

它提供的功能包括 Keep-Alive、连接池、Cookie持久化、内容自动解压、HTTP代理、SSL认证、连接超时、Session等很多特性，最重要的是它同时兼容 python2 和 python3

Python内置的urllib模块，用于访问网络资源

如果安装了Anaconda，requests就已经可用了。否则，需要在命令行下通过pip安装：
$ pip install requests

================要通过GET访问一个页面
>>> import requests
>>> r = requests.get('https://www.douban.com/') # 豆瓣首页
>>> r.status_code
200
>>> r.text
r.text
'<!DOCTYPE HTML>\n<html>\n<head>\n<meta name="description" content="提供图书、电影、音乐唱片的推荐、评论和...'


对于带参数的URL，传入一个dict作为params参数：
>>> r = requests.get('https://www.douban.com/search', params={'q': 'python', 'cat': '1001'})
>>> r.url # 实际请求的URL
'https://www.douban.com/search?q=python&cat=1001'

requests自动检测编码
>>> r.encoding
'utf-8'

无论响应是文本还是二进制内容，我们都可以用content属性获得bytes对象：
>>> r.content
b'<!DOCTYPE html>\n<html>\n<head>\n<meta http-equiv="Content-Type" content="text/html; charset=utf-8">\n...'

requests的方便之处还在于，对于特定类型的响应，例如JSON，可以直接获取：
>>> r = requests.get('https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20weather.forecast%20where%20woeid%20%3D%202151330&format=json')
>>> r.json()
{'query': {'count': 1, 'created': '2017-11-17T07:14:12Z', ...

需要传入HTTP Header时，我们传入一个dict作为headers参数：
>>> r = requests.get('https://www.douban.com/', headers={'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit'})
>>> r.text
'<!DOCTYPE html>\n<html>\n<head>\n<meta charset="UTF-8">\n <title>豆瓣(手机版)</title>...'

要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据：
>>> r = requests.post('https://accounts.douban.com/login', data={'form_email': 'abc@example.com', 'form_password': '123456'})

requests默认使用application/x-www-form-urlencoded对POST数据编码。如果要传递JSON数据，可以直接传入json参数：
params = {'key': 'value'}
r = requests.post(url, json=params) # 内部自动序列化为JSON

上传文件需要更复杂的编码格式，但是requests把它简化成files参数：
>>> upload_files = {'file': open('report.xls', 'rb')}
>>> r = requests.post(url, files=upload_files)
在读取文件时，注意务必使用'rb'即二进制模式读取，这样获取的bytes长度才是文件的长度


除了能轻松获取响应内容外，requests对获取HTTP响应的其他信息也非常简单。例如，获取响应头：
>>> r.headers
{Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Content-Encoding': 'gzip', ...}
>>> r.headers['Content-Type']
'text/html; charset=utf-8'

requests对Cookie做了特殊处理，使得我们不必解析Cookie就可以轻松获取指定的Cookie：
>>> r.cookies['ts']
'example_cookie_12345'

要在请求中传入Cookie，只需准备一个dict传入cookies参数：
>>> cs = {'token': '12345', 'status': 'working'}
>>> r = requests.get(url, cookies=cs)

要指定超时，传入以秒为单位的timeout参数：
>>> r = requests.get(url, timeout=2.5) # 2.5秒后超时

====================================Get
response = requests.get("https://foofish.net")

# 状态码
>>> response.status_code
200
 
# 原因短语
>>> response.reason
'OK'
 
# 响应首部
>>> for name,value in response.headers.items():
...     print("%s:%s" % (name, value))
...
Content-Encoding:gzip
Server:nginx/1.10.2
Date:Thu, 06 Apr 2017 16:28:01 GMT
 
# 响应内容
>>> response.content

====================================Post
r = requests.post('http://httpbin.org/post', data = {'key':'value'})

====================================构建请求查询参数
很多URL都带有很长一串参数，我们称这些参数为URL的查询参数，用”?”附加在URL链接后面，多个参数之间用”&”隔开，比如：http://fav.foofish.net/?p=4&s=20 ，现在你可以用字典来构建查询参数：

>>> args = {"p": 4, "s": 20}
>>> response = requests.get("http://fav.foofish.net", params = args)
>>> response.url
'http://fav.foofish.net/?p=4&s=2'

====================================构建请求首部 Headers
r = requests.get(url, headers={'user-agent': 'Mozilla/5.0'})

====================================构建 POST 请求数据
如果服务器要求发送的数据是表单数据，则可以指定关键字参数 data，如果要求传递 json 格式字符串参数，则可以使用json关键字参数，参数的值都可以字典的形式传过去。

作为表单数据传输给服务器
>>> payload = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.post("http://httpbin.org/post", data=payload)

作为 json 格式的字符串格式传输给服务器
>>> import json
>>> url = 'http://httpbin.org/post'
>>> payload = {'some': 'data'}
>>> r = requests.post(url, json=payload)

====================================Response中的响应体
与响应体相关的属性有：content、text、json()。
content 是 byte 类型，适合直接将内容保存到文件系统或者传输到网络中

>>> r = requests.get("https://pic1.zhimg.com/v2-2e92ebadb4a967829dcd7d05908ccab0_b.jpg")
>>> type(r.content)
<class 'bytes'>
# 另存为 test.jpg
>>> with open("test.jpg", "wb") as f:
...     f.write(r.content)


>>> r = requests.get('https://www.v2ex.com/api/topics/hot.json')
>>> r.json()

====================================代理设置
当爬虫频繁地对服务器进行抓取内容时，很容易被服务器屏蔽掉，因此要想继续顺利的进行爬取数据，使用代理是明智的选择。如果你想爬取墙外的数据，同样设置代理可以解决问题，requests 完美支持代理。

import requests
 
proxies = {
  'http': 'http://10.10.1.10:3128',
  'https': 'http://10.10.1.10:1080',
}
 
requests.get('http://example.org', proxies=proxies)

====================================超时设置
requests 发送请求时，默认请求下线程一直阻塞，直到有响应返回才处理后面的逻辑。如果遇到服务器没有响应的情况时，问题就变得很严重了，它将导致整个应用程序一直处于阻塞状态而没法处理其他请求。

>>> import requests
>>> r = requests.get("http://www.google.coma")
...一直阻塞中
正确的方式的是给每个请求显示地指定一个超时时间。

>>> r = requests.get("http://www.google.coma", timeout=5)
5秒后报错
Traceback (most recent call last):
socket.timeout: timed out

====================================Session
为了维持客户端与服务器之间的通信状态，使用 Cookie 技术使之保持双方的通信状态。

有些网页是需要登录才能进行爬虫操作的，而登录的原理就是浏览器首次通过用户名密码登录之后，服务器给客户端发送一个随机的Cookie，下次浏览器请求其它页面时，就把刚才的 cookie 随着请求一起发送给服务器，这样服务器就知道该用户已经是登录用户。

import requests
# 构建会话
session  = requests.Session()
#　登录url
session.post(login_url, data={username, password})
#　登录后才能访问的url
r = session.get(home_url)
session.close()

构建一个session会话之后，客户端第一次发起请求登录账户，服务器自动把cookie信息保存在session对象中，发起第二次请求时requests 自动把session中的cookie信息发送给服务器，使之保持通信状态

====================================